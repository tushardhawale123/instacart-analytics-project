{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, avg, desc, rank, dense_rank\n",
        "from pyspark.sql.window import Window\n",
        "import os\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Instacart Data Analysis\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define paths\n",
        "DATA_PATH = '/home/jovyan/work/1_data/processed/'\n",
        "OUTPUT_PATH = '/home/jovyan/work/1_data/spark_output/'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Load datasets\n",
        "products = spark.read.csv(f\"{DATA_PATH}products_cleaned.csv\", header=True, inferSchema=True)\n",
        "orders = spark.read.csv(f\"{DATA_PATH}orders_cleaned.csv\", header=True, inferSchema=True)\n",
        "order_products_prior = spark.read.csv(f\"{DATA_PATH}order_products_prior_cleaned.csv\", header=True, inferSchema=True)\n",
        "order_products_train = spark.read.csv(f\"{DATA_PATH}order_products_train_cleaned.csv\", header=True, inferSchema=True)\n",
        "aisles = spark.read.csv(f\"{DATA_PATH}aisles_cleaned.csv\", header=True, inferSchema=True)\n",
        "departments = spark.read.csv(f\"{DATA_PATH}departments_cleaned.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Register temporary views\n",
        "products.createOrReplaceTempView(\"products\")\n",
        "orders.createOrReplaceTempView(\"orders\")\n",
        "order_products_prior.createOrReplaceTempView(\"order_products_prior\")\n",
        "order_products_train.createOrReplaceTempView(\"order_products_train\")\n",
        "aisles.createOrReplaceTempView(\"aisles\")\n",
        "departments.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "# Combine order products data\n",
        "order_products = order_products_prior.withColumn(\"data_source\", col(\"reordered\").cast(\"string\")) \\\n",
        "    .union(order_products_train.withColumn(\"data_source\", col(\"reordered\").cast(\"string\")))\n",
        "\n",
        "# Analysis 1: Most popular products\n",
        "print(\"Calculating most popular products...\")\n",
        "popular_products = order_products.groupBy(\"product_id\") \\\n",
        "    .agg(count(\"*\").alias(\"order_count\")) \\\n",
        "    .join(products, \"product_id\") \\\n",
        "    .join(aisles, \"aisle_id\") \\\n",
        "    .join(departments, \"department_id\") \\\n",
        "    .select(\"product_id\", \"product_name\", \"aisle\", \"department\", \"order_count\") \\\n",
        "    .orderBy(desc(\"order_count\"))\n",
        "\n",
        "popular_products.write.csv(f\"{OUTPUT_PATH}popular_products\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# Analysis 2: Reorder rates by department\n",
        "print(\"Calculating reorder rates by department...\")\n",
        "reorder_rates = order_products.groupBy(\"product_id\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_orders\"),\n",
        "        sum(col(\"reordered\").cast(\"int\")).alias(\"reorder_count\")\n",
        "    ) \\\n",
        "    .withColumn(\"reorder_rate\", col(\"reorder_count\") / col(\"total_orders\")) \\\n",
        "    .join(products, \"product_id\") \\\n",
        "    .join(departments, \"department_id\") \\\n",
        "    .groupBy(\"department_id\", \"department\") \\\n",
        "    .agg(avg(\"reorder_rate\").alias(\"avg_reorder_rate\")) \\\n",
        "    .orderBy(desc(\"avg_reorder_rate\"))\n",
        "\n",
        "reorder_rates.write.csv(f\"{OUTPUT_PATH}reorder_rates_by_department\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# Analysis 3: Order patterns by hour of day\n",
        "print(\"Analyzing order patterns by hour of day...\")\n",
        "hour_patterns = orders.groupBy(\"order_hour_of_day\") \\\n",
        "    .agg(count(\"*\").alias(\"order_count\")) \\\n",
        "    .orderBy(\"order_hour_of_day\")\n",
        "\n",
        "hour_patterns.write.csv(f\"{OUTPUT_PATH}order_hour_patterns\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# Analysis 4: User purchase frequency\n",
        "print(\"Analyzing user purchase patterns...\")\n",
        "user_frequency = orders.groupBy(\"user_id\") \\\n",
        "    .agg(count(\"*\").alias(\"order_count\")) \\\n",
        "    .orderBy(desc(\"order_count\"))\n",
        "\n",
        "user_frequency.write.csv(f\"{OUTPUT_PATH}user_frequency\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# Analysis 5: Top products by aisle\n",
        "print(\"Finding top products by aisle...\")\n",
        "windowSpec = Window.partitionBy(\"aisle_id\").orderBy(desc(\"product_count\"))\n",
        "\n",
        "top_aisle_products = order_products.groupBy(\"product_id\") \\\n",
        "    .agg(count(\"*\").alias(\"product_count\")) \\\n",
        "    .join(products, \"product_id\") \\\n",
        "    .join(aisles, \"aisle_id\") \\\n",
        "    .select(\"aisle_id\", \"aisle\", \"product_id\", \"product_name\", \"product_count\") \\\n",
        "    .withColumn(\"rank\", dense_rank().over(windowSpec)) \\\n",
        "    .filter(col(\"rank\") <= 5) \\\n",
        "    .orderBy(\"aisle\", \"rank\")\n",
        "\n",
        "top_aisle_products.write.csv(f\"{OUTPUT_PATH}top_aisle_products\", header=True, mode=\"overwrite\")\n",
        "\n",
        "print(f\"Spark processing complete. Results saved to {OUTPUT_PATH}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (PySpark)",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}